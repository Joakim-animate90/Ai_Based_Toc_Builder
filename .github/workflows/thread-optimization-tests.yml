name: Thread Optimization Tests

on:
  push:
    paths:
      - 'app/utils/process_image_thread.py'
      - 'tests/utils/test_process_image_thread.py'
  pull_request:
    paths:
      - 'app/utils/process_image_thread.py'
      - 'tests/utils/test_process_image_thread.py'
  # Run weekly to catch regressions
  schedule:
    - cron: '0 0 * * 0'  # Every Sunday at midnight

jobs:
  thread-tests:
    name: Thread Optimization Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ["3.8", "3.10"]  # Test oldest and newest supported
        test-type: ["unit", "performance"]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-threads-${{ hashFiles('**/requirements.txt') }}-${{ matrix.python-version }}
          restore-keys: |
            ${{ runner.os }}-pip-threads-${{ matrix.python-version }}-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-benchmark
          pip install -r requirements.txt
          
      - name: Install system dependencies for PDF processing
        run: |
          sudo apt-get update
          sudo apt-get install -y poppler-utils libmupdf-dev
      
      - name: Configure thread testing environment
        run: |
          # Ensure tests run in a controlled environment for thread testing
          export PYTHONUNBUFFERED=1
          # Set limits to verify behavior with different CPU configurations
          if [ "${{ matrix.test-type }}" = "performance" ]; then
            # For performance tests, we want to use all CPUs
            echo "THREAD_COUNT=0" >> $GITHUB_ENV
          else
            # For unit tests, restrict to 2 CPUs to ensure predictable behavior
            echo "THREAD_COUNT=2" >> $GITHUB_ENV
          fi
      
      - name: Run thread-specific unit tests
        if: matrix.test-type == 'unit'
        run: |
          pytest tests/utils/test_process_image_thread.py -v --thread-count=$THREAD_COUNT
      
      - name: Run thread performance benchmarks
        if: matrix.test-type == 'performance'
        run: |
          # Create test PDFs of different sizes
          mkdir -p test_data
          python -c "
          import fitz
          # Create a small test PDF
          doc = fitz.open()
          for i in range(5):
              page = doc.new_page()
              page.insert_text((50, 50), f'Test page {i}')
          doc.save('test_data/small.pdf')
          # Create a larger test PDF
          doc = fitz.open()
          for i in range(20):
              page = doc.new_page()
              page.insert_text((50, 50), f'Test page {i}')
          doc.save('test_data/large.pdf')
          "
          
          # Run benchmarks with different thread counts
          pytest tests/utils/test_process_image_thread.py::test_benchmark_pdf_conversion -v \
            --benchmark-sort=name \
            --benchmark-columns=min,max,mean,rounds \
            --benchmark-group-by=param \
            --benchmark-save=thread_benchmark
      
      - name: Upload benchmark results
        if: matrix.test-type == 'performance'
        uses: actions/upload-artifact@v4
        with:
          name: thread-benchmark-py${{ matrix.python-version }}
          path: .benchmarks/
          
      - name: Generate benchmark comparison (if previous data exists)
        if: matrix.test-type == 'performance'
        continue-on-error: true
        run: |
          # Try to compare with previous benchmarks if they exist
          if [ -d ".benchmarks" ]; then
            pytest-benchmark compare --csv=comparison.csv || echo "No previous benchmark to compare with"
            if [ -f "comparison.csv" ]; then
              echo "## Thread Performance Comparison" > benchmark_report.md
              echo "\`\`\`" >> benchmark_report.md
              cat comparison.csv >> benchmark_report.md
              echo "\`\`\`" >> benchmark_report.md
            fi
          fi
      
      - name: Upload comparison report
        if: matrix.test-type == 'performance' && success()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-py${{ matrix.python-version }}
          path: benchmark_report.md
          if-no-files-found: ignore
          
  # Notifications removed as requested
